{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "text_n_img_preprocessing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL902Ueis6gE",
        "outputId": "85e4b75c-ca1c-46fd-9731-a5d86d9ac0fc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing import image,sequence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxwTQmP9s6gI",
        "outputId": "7d8976f1-7e05-4d43-9334-78f99a806aa1"
      },
      "source": [
        "data = pd.read_csv('./traindat.txt', delimiter='\\t',header=None)\n",
        "print(data.shape)\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2513260012_03d33305cf.jpg</td>\n",
              "      <td>&lt;start&gt; A black dog is running after a white d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2513260012_03d33305cf.jpg</td>\n",
              "      <td>&lt;start&gt; Black dog chasing brown dog through sn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2513260012_03d33305cf.jpg</td>\n",
              "      <td>&lt;start&gt; Two dogs chase each other across the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2513260012_03d33305cf.jpg</td>\n",
              "      <td>&lt;start&gt; Two dogs play together in the snow . &lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2513260012_03d33305cf.jpg</td>\n",
              "      <td>&lt;start&gt; Two dogs running through a low lying b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           0  \\\n",
              "0  2513260012_03d33305cf.jpg   \n",
              "1  2513260012_03d33305cf.jpg   \n",
              "2  2513260012_03d33305cf.jpg   \n",
              "3  2513260012_03d33305cf.jpg   \n",
              "4  2513260012_03d33305cf.jpg   \n",
              "\n",
              "                                                   1  \n",
              "0  <start> A black dog is running after a white d...  \n",
              "1  <start> Black dog chasing brown dog through sn...  \n",
              "2  <start> Two dogs chase each other across the s...  \n",
              "3  <start> Two dogs play together in the snow . <...  \n",
              "4  <start> Two dogs running through a low lying b...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVOCoVR2s6gJ",
        "outputId": "a4079ea4-fd6a-43df-c63b-1abcb1e18cd3"
      },
      "source": [
        "sentences = []\n",
        "for ix in range(data.shape[0]):\n",
        "    sentences.append(data.loc[ix][1])\n",
        "print(len(sentences))\n",
        "# print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE1xD5c8s6gJ",
        "outputId": "d878c5ef-029d-4a7a-d485-493c846da46d"
      },
      "source": [
        "words = [i.split() for i in sentences]\n",
        "unique = []\n",
        "max_len = 0 \n",
        "for i in words:\n",
        "    unique.extend(i)\n",
        "    if(len(i)>max_len):\n",
        "        max_len = len(i)\n",
        "\n",
        "unique = list(set(unique))\n",
        "print(len(unique))\n",
        "print(max_len)\n",
        "vocab_size = len(unique)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8253\n",
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o58b2Jzns6gK",
        "outputId": "46230be2-87c5-4dc9-a6fe-e417a455ce6c"
      },
      "source": [
        "#Vectorization\n",
        "word_2_indices = { val:index for index, val in enumerate(unique)}\n",
        "indices_2_word = { index:val for index, val in enumerate(unique)}\n",
        "temp = indices_2_word[0]\n",
        "word_2_indices[temp]= vocab_size\n",
        "word_2_indices['EMP'] = 0\n",
        "indices_2_word[0] = 'EMP'\n",
        "indices_2_word[vocab_size]= temp\n",
        "vocab_size = len(word_2_indices)\n",
        "print(vocab_size)\n",
        "\n",
        "with open('./word_2_indices.p','wb') as f1:\n",
        "    pickle.dump(word_2_indices,f1)\n",
        "with open('./indices_2_words.p','wb') as f2:\n",
        "    pickle.dump(indices_2_word,f2)\n",
        "f1.close()\n",
        "f2.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g261sAd-s6gL",
        "outputId": "3957e112-9011-42a5-bc1e-46b8d5ccbf88"
      },
      "source": [
        "padded_sequences, subsequent_words = [], []\n",
        "\n",
        "for ix in range(len(sentences)):\n",
        "    partial_seqs = []\n",
        "    next_words = []\n",
        "    text = sentences[ix].split()\n",
        "    text = [word_2_indices[i] for i in text]\n",
        "    for i in range(1, len(text)):\n",
        "        partial_seqs.append(text[:i])\n",
        "        next_words.append(text[i])\n",
        "    \n",
        "    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')\n",
        "    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)\n",
        "    \n",
        "    #Vectorization\n",
        "    for i,next_word in enumerate(next_words):\n",
        "        next_words_1hot[i, next_word] = 1\n",
        "        \n",
        "    padded_sequences.append(padded_partial_seqs)\n",
        "    subsequent_words.append(next_words_1hot)\n",
        "    \n",
        "padded_sequences = np.asarray(padded_sequences)\n",
        "subsequent_words = np.asarray(subsequent_words)\n",
        "\n",
        "print(padded_sequences.shape)\n",
        "print(subsequent_words.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000,)\n",
            "(30000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbS8nkvts6gL",
        "outputId": "71e81947-1282-46ec-9c75-0a80603f0871"
      },
      "source": [
        "print(padded_sequences[0].shape)\n",
        "print(subsequent_words[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 40)\n",
            "(14, 8254)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dQwP1qes6gN",
        "outputId": "3c98ad54-9740-411c-ff2c-aeadd79fdf46"
      },
      "source": [
        "for ix in range(len(padded_sequences[0])):\n",
        "    for iy in range(max_len):\n",
        "        print(indices_2_word[padded_sequences[0][ix][iy]],end=' ')\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(len(padded_sequences[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running after EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running after a EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running after a white EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running after a white dog EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running after a white dog in EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running after a white dog in the EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running after a white dog in the snow EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "<start> A black dog is running after a white dog in the snow . EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP EMP \n",
            "\n",
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1S04CU-s6gO",
        "outputId": "b143dfbd-7dd6-4f04-a0c0-34332459f1a8"
      },
      "source": [
        "with open('./train_encoded_images.p', 'rb') as f:\n",
        "        encoded_images = pickle.load(f)\n",
        "\n",
        "imgs = []\n",
        "for ix in range(data.shape[0]):\n",
        "    if data.loc[ix, 0] in encoded_images.keys():\n",
        "        imgs.append(list(encoded_images[data.loc[ix, 0]]))\n",
        "\n",
        "    else:\n",
        "        imgs.append(list([]))\n",
        "images = []\n",
        "image_names = []\n",
        "\n",
        "imgs = np.asarray(imgs)\n",
        "print(imgs.shape)\n",
        "\n",
        "captions = np.zeros([0, max_len])\n",
        "print(captions.shape)\n",
        "next_words = np.zeros([0, vocab_size])\n",
        "print(next_words.shape)\n",
        "for ix in range(2000):\n",
        "    if(len(imgs[ix])!=0):\n",
        "        captions = np.concatenate([captions, padded_sequences[ix]])\n",
        "        next_words = np.concatenate([next_words, subsequent_words[ix]])\n",
        "        for iy in range(padded_sequences[ix].shape[0]):\n",
        "            image_names.append(data.loc[ix][0])\n",
        "            images.append(imgs[ix])\n",
        "        \n",
        "    if(ix%100==0):\n",
        "        print(ix,end=' ')\n",
        "image_names = np.asarray(image_names)\n",
        "images = np.asarray(images)\n",
        "        \n",
        "np.save(\"./captions.npy\", captions)\n",
        "np.save(\"./next_words.npy\", next_words)\n",
        "np.save(\"./image_names.npy\", image_names)\n",
        "np.save(\"./images.npy\", images)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000,)\n",
            "(0, 40)\n",
            "(0, 8254)\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYjFLk3Qs6gP",
        "outputId": "d9ebd104-7d2e-40cb-e1f3-199af34131c8"
      },
      "source": [
        "print(captions.shape)\n",
        "print(next_words.shape)\n",
        "print(len(image_names))\n",
        "print(len(images))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25493, 40)\n",
            "(25493, 8254)\n",
            "25493\n",
            "25493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACuZXpwis6gQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}